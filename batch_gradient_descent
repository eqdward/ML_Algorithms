# -*- coding: utf-8 -*-


import numpy as np

class linear_regression_gd:
    """梯度下降法求解线性回归"""
    
    def __init__(self):
        """初始化属性"""
        self._theta = None
        self.intercept_ = None   # 截距/常数
        self.coef_ = None   # 系数
    

    def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):
        assert X_train.shape[0] == y_train.shape[0], "X和y的size应一致"
    
        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = self._gradient_descent(X_b, y_train, initial_theta, eta, n_iters)
        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]
        
        return self
    
    
    def _J(self, theta, X_b, y):
        """损失函数"""
        try:
            return np.sum((y - X_b.dot(theta))**2 / len(y))
        except:
            return float(np.inf)
    
    
    def _dJ(self, theta, X_b, y):
        """损失函数的梯度公式"""
        return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)
    
    
    def _gradient_descent(self, X_b, y, initial_theta, eta, n_iters, epsilon=1e-8):
        """梯度下降法求损失函数最小时的theta"""
        theta = initial_theta
        cur_iter = 0
        
        while cur_iter < n_iters:
            gradient = self._dJ(theta, X_b, y)
            last_theta = theta
            theta = theta - eta*gradient            
            if abs(self._J(theta, X_b, y) - self._J(last_theta, X_b, y)) < epsilon:
                break
            
            cur_iter += 1
        
        return theta 
    
    def predict(self, X_test):
        assert self.intercept_ is not None and self.coef_ is not None, "必须先fit"
        assert X_test.shape[1] == len(self.coef_), "验证集的自变量数量与训练集的自变量数量应一致"
        
        X_b = np.hstack([np.ones((len(X_test), 1)), X_test])
        return X_b.dot(self._theta)
    
    
    def r2_score(self, X_test, y_predict):
        """计算y_test和y_predict之间的R Squared"""
        assert len(X_test) == len(y_predict), "真实值和预测值的数据量大小应一致"
        y_predict = self.predict(X_test)
        return 1 - np.sum((y_test - y_predict) ** 2) / len(y_test) / np.var(y_test)
    
    
    def __repr__(self):
        return "LinearRegression with Gradient Descent"


if __name__ == "__main__":
    
    """应用示例"""
    
    from sklearn.model_selection import train_test_split
    from sklearn import datasets
    
    boston = datasets.load_boston()
    X = boston.data
    y = boston.target
    
    X = X[y<50]
    y = y[y<50]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)
      
    
    """使用一般求解方法"""
    from sklearn.linear_model import LinearRegression
    lr = LinearRegression()
    
    lr.fit(X_train, y_train)
    lr.coef_
    lr.intercept_
    y_predict = lr.predict(X_test)
    lr.score(X_test, y_test)
    
    def r2_score(y_test, y_predict):
        """计算y_test和y_predict之间的R Squared"""
        assert len(y_test) == len(y_predict), "真实值和预测值的数据量大小应一致"
        return 1 - np.sum((y_test - y_predict) ** 2) / len(y_test) / np.var(y_test)
    
    r2_score(y_test, y_predict)
    
    
    """使用梯度下降法"""
    from sklearn.preprocessing import StandardScaler
    standardScaler = StandardScaler()
    standardScaler.fit(X_train)    
    X_train_std = standardScaler.transform(X_train)
    X_test_std = standardScaler.transform(X_test)
    
    lr2 = linear_regression_gd()
    lr2.fit(X_train_std, y_train)
    lr2.r2_score(X_test_std, y_test)
