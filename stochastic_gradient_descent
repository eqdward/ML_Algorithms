# -*- coding: utf-8 -*-

import numpy as np

class linear_regression_sgd:
    """随机梯度下降法求解线性回归"""
    
    def __init__(self):
        """初始化属性"""
        self._theta = None
        self.intercept_ = None   # 截距/常数
        self.coef_ = None   # 系数
    

    def fit(self, X_train, y_train, n_iters=50, t0=5, t1=50):
        assert X_train.shape[0] == y_train.shape[0], "X和y的size应一致"
        assert n_iters >= 1
    
        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = self._sgd(X_b, y_train, initial_theta, n_iters, t0, t1)
        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]
        
        return self
    
    
    def _J(self, theta, X_b, y):
        """损失函数"""
        try:
            return np.sum((y - X_b.dot(theta))**2 / len(y))
        except:
            return float(np.inf)
    
    
    def _dJ_sgd(self,theta, X_b_i, y_i):
        """损失函数的梯度公式"""
        return X_b_i.T.dot(X_b_i.dot(theta) - y_i)*2. 
    

    def _sgd(self, X_b, y, initial_theta, n_iters, t0, t1):
        """随机梯度下降"""
        m = len(X_b)
        theta = initial_theta
    
        for i_iter in range(n_iters):
            """随机打乱原数据，相当于随机取值"""
            rand_index = np.random.permutation(m)
            X_b_new = X_b[rand_index,:]
            y_new = y[rand_index]
            for i in range(m):
                gradient = self._dJ_sgd(theta, X_b_new[i], y_new[i])
                eta = t0 / (i_iter*m + i + t1)
                theta = theta - eta * gradient
    
        return theta


    def predict(self, X_test):
        assert self.intercept_ is not None and self.coef_ is not None, "必须先fit"
        assert X_test.shape[1] == len(self.coef_), "验证集的自变量数量与训练集的自变量数量应一致"
        
        X_b = np.hstack([np.ones((len(X_test), 1)), X_test])
        return X_b.dot(self._theta)
    
    
    def r2_score(self, X_test, y_predict):
        """计算y_test和y_predict之间的R Squared"""
        assert len(X_test) == len(y_predict), "真实值和预测值的数据量大小应一致"
        y_predict = self.predict(X_test)
        return 1 - np.sum((y_test - y_predict) ** 2) / len(y_test) / np.var(y_test)
    
    
    def __repr__(self):
        return "LinearRegression with sgd"
    

if __name__ == "__main__":
    
    """应用示例"""
    
    from sklearn.model_selection import train_test_split
    from sklearn import datasets
    from sklearn.preprocessing import StandardScaler

    boston = datasets.load_boston()
    X = boston.data
    y = boston.target
    
    X = X[y<50]
    y = y[y<50]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)
      
    standardScaler = StandardScaler()
    standardScaler.fit(X_train)    
    X_train_std = standardScaler.transform(X_train)
    X_test_std = standardScaler.transform(X_test) 
    
    lr = linear_regression_sgd()
    lr.fit(X_train_std, y_train, n_iters=100)
    lr.r2_score(X_test_std, y_test)
    
    
    
    """sklearn中的SGD"""
    from sklearn.linear_model import SGDRegressor
    
    sgd_reg = SGDRegressor(n_iter=100)  # 默认n_iters=5
    sgd_reg.fit(X_train_std, y_train)
    sgd_reg.score(X_test_std, y_test)
    
    
    
