"""0. 示例数据"""
dataSet=np.array([[0, 0, 0, 0, 0],
                  [0, 0, 0, 1, 0],
                  [0, 1, 0, 1, 1],
                  [0, 1, 1, 0, 1],
                  [0, 0, 0, 0, 0],
                  [1, 0, 0, 0, 0],
                  [1, 0, 0, 1, 0],
                  [1, 1, 1, 1, 1],
                  [1, 0, 1, 2, 1],
                  [1, 0, 1, 2, 1],
                  [2, 0, 1, 2, 1],
                  [2, 0, 1, 1, 1],
                  [2, 1, 0, 1, 1],
                  [2, 1, 0, 2, 1],
                  [2, 0, 0, 0, 0]])
featName = ['年龄','有工作','有自己的房子','信贷情况']

X = dataSet[:,:-1]   # 特征矩阵
y = dataSet[:,-1]   #  类向量

"""1. 信息熵的最优划分"""
import numpy as np
from collections import Counter
from math import log

def calEntropy(y):
    """计算经验熵"""
    counter = Counter(y)   # 对y中取值分类，并统计每一类取值的计数，返回一个字典
    entropy = 0
    for num in counter.values():
        p = num / len(y)   # 计算每一类取值的发生概率
        entropy += -p*log(p)
    return entropy

def split(X, y, feat, value):
    """选择X中的某个特征feat，按照value，对数据集进行划分"""
    index_a = (X[:,feat] <= value)   # 小于value的索引
    index_b = (X[:,feat] > value)   # 大于value的索引
    return X[index_a], X[index_b], y[index_a], y[index_b]

def try_split(X, y):

    """数据集划分，逐个特征、逐个值进行尝试"""
    """初始化划分参数"""
    best_entropy = float('inf')
    best_feat = -1
    best_value = -1
    
    for feat in range(X.shape[1]):   # 按列（特征）循环
        sort_index = np.argsort(X[:,feat])   # 对该列数据的排序，返回顺序序号
        
        for i in range(1, len(X)):
            if X[sort_index[i-1], feat] != X[sort_index[i], feat]:   #判断数据类别是否一致，不一致才尝试划分
                        
                value = (X[sort_index[i-1], feat] + X[sort_index[i], feat]) / 2   #取两数均值作为划分值
                X1, X2, y1, y2 = split(X, y, feat, value)
                entropy = calEntropy(y1) + calEntropy(y2)   # 计算划分的经验熵

                if entropy < best_entropy:
                    best_entropy, best_feat, best_value = entropy, feat, value
    return best_entropy, best_feat, best_value
    
best_entropy, best_feat, best_value = try_split(X, y)


"""2. 信息增益的最优划分"""
#import numpy as np
#from math import log

def calEntropy2(dataSet):
    """计算经验熵"""

    labelCounts={}   # 记录类及取值数量
    
    for feat in dataSet:   # 取dataSet中的每一行
        currentLabel=feat[-1]   # 取每一行的最后一个元素，即类
        if currentLabel not in labelCounts.keys():   # 如果类没有在labelCounts中进行统计，则加入labelCounts中
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
        
    entropy = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key])/len(dataSet)

        entropy -= prob*np.log2(prob)
    return entropy


def currentConditionSet(dataSet, cutFeatIndex, category):
    """为计算条件熵，需先获取条件下的数据子集"""
    tempFeatSets = []
    for featVec in dataSet:
        if featVec[cutFeatIndex] == category:
            tempFeatSet = np.append(featVec[:cutFeatIndex],featVec[cutFeatIndex+1:])
            tempFeatSets.append(tempFeatSet)
    return tempFeatSets


def calConditionalEnt(dataSet, cutFeatIndex):
    """计算当前划分条件下的条件熵"""
    conditionalEnt = 0
    categories = set(dataSet[:,cutFeatIndex])   # 用集合来获取分割特征的取值
    
    for category in categories:   # 对特征的每一种取值计算条件熵并求和
        subConditionalSet = currentConditionSet(dataSet, cutFeatIndex, category)
        prob = len(subConditionalSet) / float(len(dataSet))   # 子集发生的概率
        conditionalEnt += prob * calEntropy2(subConditionalSet)
 
    return conditionalEnt


def calInfoGain(baseEntropy, dataSet, cutFeatIndex):
    """计算信息增益"""
    conditionalEnt = calConditionalEnt(dataSet,cutFeatIndex)   # 条件熵
    infoGain = baseEntropy - conditionalEnt   # 信息增益
    
    print("第%d个特征的增益为%.3f" % (cutFeatIndex, infoGain))
    return infoGain


def optimalPartition(dataSet):
    """使用信息增益寻找最佳划分"""
    bestInfoGain = -1
    bestFeatVec = -1
    baseEntropy = calEntropy2(dataSet)
    
    for cutFeatIndex in range(dataSet.shape[1]-1):   # 遍历每一个特征
        infoGain = calInfoGain(baseEntropy, dataSet, cutFeatIndex)
        
        if infoGain > bestInfoGain:
            bestInfoGain = infoGain
            bestFeatVec = cutFeatIndex
    
    print("最佳的划分为第%d个特征，是“%s”，信息增益为%.3f" % (bestFeatVec,featName[bestFeatVec],bestInfoGain))
            
optimalPartition(dataSet)


"""3. 信息增益率的最优划分"""
def calPenaltyPara(dataSet, cutFeatIndex):
    """计算惩罚系数"""
    penaltyItem = 1
    categories = set(dataSet[:,cutFeatIndex])   # 用集合来获取分割特征的取值
    
    for category in categories:
        subConditionalSet = currentConditionSet(dataSet, cutFeatIndex, category)
        prob = len(subConditionalSet) / float(len(dataSet))   # 子集发生的概率
        penaltyItem += -prob * log(prob, 2)
    
    return penaltyItem


def calInfoGainRate(baseEntropy, dataSet, cutFeatIndex):
    """计算信息增益率"""
    infoGain = calInfoGain(baseEntropy, dataSet, cutFeatIndex)
    penaltyItem = calPenaltyPara(dataSet, cutFeatIndex)
    infoGainRate = infoGain / penaltyItem
    
    print("第%d个特征的增益率为%.3f" % (cutFeatIndex, infoGainRate))
    return infoGainRate


def optimalPartition(dataSet):
    bestInfoGainRatio = 0.0   # 初始化最大信息增益率
    bestFeat = -1   # 最佳划分特征
    baseEntropy = calEntropy2(dataSet)
    
    for cutFeatIndex in range(dataSet.shape[1]-1):
        infoGainRatio = calInfoGainRate(baseEntropy, dataSet, cutFeatIndex)
        if infoGainRatio > bestInfoGainRatio:
            bestInfoGainRatio = infoGainRatio
            bestFeat = cutFeatIndex
    
    print("最佳的划分为第%d个特征，是”%s“，信息增益率为%.3f" % (bestFeat,featName[bestFeat],bestInfoGainRatio))
    return

optimalPartition(dataSet)


"""4. 基尼系数的最优划分"""
def calGini(y):
    """计算一维向量y的基尼系数"""
    counter = Counter(y.flatten())
    gini = 1
    for num in counter.values():
        p = num / len(y)
        gini -= p**2
    return gini

def try_split(X, y):
    """根据基尼系数寻找最优划分"""
    bestGini = float('inf')
    bestFeat = -1
    bestValue = -1
    
    for feat in range(X.shape[1]):
        sort_index = np.argsort(X[:,feat])
        
        for i in range(1, len(X)):
            if X[sort_index[i-1], feat] != X[sort_index[i], feat]:
                value = (X[sort_index[i-1], feat] + X[sort_index[i], feat]) / 2
                X1, X2, y1, y2 = split(X, y, feat, value)
                gini = calGini(y1) + calGini(y2)
                if gini < bestGini:
                    bestGini, bestFeat, bestValue = gini, feat, value
    return bestGini, bestFeat, bestValue
    

bestGini, bestFeatVec, bestValue = try_split(X, y)
print("最优基尼系数：", bestGini)
print("在哪个维度上进行划分：", bestFeatVec)
print("在哪个值上进行划分：", bestValue)
